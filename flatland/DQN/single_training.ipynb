{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import getopt\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from collections import deque\n",
    "# make sure the root path is in system path\n",
    "from pathlib import Path\n",
    "\n",
    "from flatland.envs.malfunction_generators import malfunction_from_params\n",
    "# base_dir = Path(__file__).resolve().parent.parent\n",
    "# sys.path.append(str(base_dir))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from double_duelling_dqn import Agent\n",
    "from observation_utils import normalize_observation\n",
    "\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.schedule_generators import sparse_schedule_generator\n",
    "from flatland.utils.rendertools import RenderTool\n",
    "from flatland.envs.observations import TreeObsForRailEnv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_trials):\n",
    "\n",
    "    #fix the randomness\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "\n",
    "    #parameters for the environment\n",
    "    x_dim = 35\n",
    "    y_dim = 35\n",
    "    n_agents = 1\n",
    "    \n",
    "    # Use the malfunction generator to break agents from time to time\n",
    "    stochastic_data = {'malfunction_rate': 8000,  # Rate of malfunction occurence of single agent\n",
    "                       'min_duration': 15,  # Minimal duration of malfunction\n",
    "                       'max_duration': 50  # Max duration of malfunction\n",
    "                       }\n",
    "\n",
    "    # Custom observation builder\n",
    "    TreeObservation = TreeObsForRailEnv(max_depth=2)\n",
    "\n",
    "    # Different agent types (trains) with different speeds.\n",
    "    speed_ration_map = {1.: 0.,  # Fast passenger train\n",
    "                        1. / 2.: 1.0,  # Fast freight train\n",
    "                        1. / 3.: 0.0,  # Slow commuter train\n",
    "                        1. / 4.: 0.0}  # Slow freight train\n",
    "\n",
    "\n",
    "    env = RailEnv(width=x_dim,\n",
    "                  height=y_dim,\n",
    "                  rail_generator=sparse_rail_generator(max_num_cities=3,\n",
    "                                                       # Number of cities in map (where train stations are)\n",
    "                                                       seed=1,  # Random seed\n",
    "                                                       grid_mode=False,\n",
    "                                                       max_rails_between_cities=2,\n",
    "                                                       max_rails_in_city=3),\n",
    "                  schedule_generator=sparse_schedule_generator(),\n",
    "                  number_of_agents=n_agents,\n",
    "                  malfunction_generator_and_process_data=malfunction_from_params(stochastic_data),\n",
    "                  # Malfunction data generator\n",
    "                  obs_builder_object=TreeObservation)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # After training we want to render the results so we also load a renderer\n",
    "    env_renderer = RenderTool(env, gl=\"PILSVG\", )\n",
    "    obs , info = env.reset()\n",
    "    #computing the size of the state space\n",
    "    # Given the depth of the tree observation and the number of features per node we get the following state_size\n",
    "    num_features_per_node = env.obs_builder.observation_dim\n",
    "    tree_depth = 2\n",
    "    nr_nodes = 0\n",
    "    for i in range(tree_depth + 1):\n",
    "        nr_nodes += np.power(4, i)\n",
    "    state_size = num_features_per_node * nr_nodes\n",
    "\n",
    "    # The action space of flatland is 5 discrete actions\n",
    "    action_size = 5\n",
    "\n",
    "    # And the max number of steps we want to take per episode\n",
    "    max_steps = int(3 * (env.height + env.width))\n",
    "\n",
    "    # Define training parameters\n",
    "    eps = 1.\n",
    "    eps_end = 0.005\n",
    "    eps_decay = 0.998\n",
    "\n",
    "    # And some variables to keep track of the progress\n",
    "    action_dict = dict()\n",
    "    final_action_dict = dict()\n",
    "    scores_window = deque(maxlen=100)\n",
    "    done_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    dones_list = []\n",
    "\n",
    "    action_prob = [0] * action_size\n",
    "    agent_obs = [None] * env.get_num_agents()\n",
    "    agent_next_obs = [None] * env.get_num_agents()\n",
    "    agent_obs_buffer = [None] * env.get_num_agents()\n",
    "    agent_action_buffer = [2] * env.get_num_agents()\n",
    "    cummulated_reward = np.zeros(env.get_num_agents())\n",
    "    update_values = False\n",
    "    # Now we load a Double dueling DQN agent\n",
    "    agent = Agent(state_size, action_size)\n",
    "\n",
    "    for trials in range(1, n_trials + 1):\n",
    "        # Reset environment\n",
    "        obs, info = env.reset(True, True)\n",
    "        env_renderer.reset()\n",
    "        \n",
    "        for a in range(env.get_num_agents()):\n",
    "            if obs[a]:\n",
    "                agent_obs[a] = normalize_observation(obs[a], tree_depth, observation_radius=10)\n",
    "                agent_obs_buffer[a] = agent_obs[a].copy()\n",
    "\n",
    "        # Reset score and done\n",
    "        score = 0\n",
    "        env_done = 0\n",
    "\n",
    "        # Run episode\n",
    "        for step in range(max_steps):\n",
    "            # Action\n",
    "            for a in range(env.get_num_agents()):\n",
    "                if info['action_required'][a]:\n",
    "                    # If an action is require, we want to store the obs a that step as well as the action\n",
    "                    update_values = True\n",
    "                    action = agent.act(agent_obs[a], eps=eps)\n",
    "                    action_prob[action] += 1\n",
    "                else:\n",
    "                    update_values = False\n",
    "                    action = 0\n",
    "\n",
    "                action_dict.update({a: action})\n",
    "                \n",
    "            # Environment step\n",
    "            next_obs, all_rewards, done, info = env.step(action_dict)\n",
    "            # Update replay buffer and train agent\n",
    "            for a in range(env.get_num_agents()):\n",
    "                # Only update the values when we are done or when an action was taken and thus relevant information is present\n",
    "                if update_values or done[a]:\n",
    "                    agent.step(agent_obs_buffer[a], agent_action_buffer[a], all_rewards[a],\n",
    "                               agent_obs[a], done[a])\n",
    "                    cummulated_reward[a] = 0.\n",
    "                \n",
    "                    agent_obs_buffer[a] = agent_obs[a].copy()\n",
    "                    agent_action_buffer[a] = action_dict[a]\n",
    "                \n",
    "                if next_obs[a]:\n",
    "                    agent_obs[a] = normalize_observation(next_obs[a], tree_depth, observation_radius=10)\n",
    "                \n",
    "                score += all_rewards[a] / env.get_num_agents()\n",
    "                \n",
    "            # Copy observation\n",
    "            if done['__all__']:\n",
    "                env_done = 1\n",
    "                break\n",
    "        \n",
    "        # Epsilon decay\n",
    "        eps = max(eps_end, eps_decay * eps)  # decrease epsilon\n",
    "        # Collection information about training\n",
    "        tasks_finished = 0\n",
    "        for _idx in range(env.get_num_agents()):\n",
    "            if done[_idx] == 1:\n",
    "                tasks_finished += 1\n",
    "\n",
    "        done_window.append(tasks_finished / max(1, env.get_num_agents()))\n",
    "        scores_window.append(score / max_steps)  # save most recent score\n",
    "        scores.append(np.mean(scores_window))\n",
    "        dones_list.append((np.mean(done_window)))\n",
    "\n",
    "        print(\n",
    "            '\\rTraining {} Agents on ({},{}).\\t Episode {}\\t Average Score: {:.3f}\\tDones: {:.2f}%\\tEpsilon: {:.2f} \\t Action Probabilities: \\t {}'.format(\n",
    "                env.get_num_agents(), x_dim, y_dim,\n",
    "                trials,\n",
    "                np.mean(scores_window),\n",
    "                100 * np.mean(done_window),\n",
    "                eps, action_prob / np.sum(action_prob)), end=\" \")\n",
    "\n",
    "\n",
    "        if trials % 100 == 0:\n",
    "            print(\n",
    "                '\\rTraining {} Agents on ({},{}).\\t Episode {}\\t Average Score: {:.3f}\\tDones: {:.2f}%\\tEpsilon: {:.2f} \\t Action Probabilities: \\t {}'.format(\n",
    "                    env.get_num_agents(), x_dim, y_dim,\n",
    "                    trials,\n",
    "                    np.mean(scores_window),\n",
    "                    100 * np.mean(done_window),\n",
    "                    eps, action_prob / np.sum(action_prob)))\n",
    "\n",
    "            torch.save(agent.qnetwork_local.state_dict(),\n",
    "                       './Nets/navigator_checkpoint' + str(trials) + '.pth')\n",
    "\n",
    "            action_prob = [1] * action_size\n",
    "            \n",
    "    # Plot overall training progress at the end\n",
    "    plt.plot(scores)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Agents on (35,35).\t Episode 100\t Average Score: -0.711\tDones: 48.00%\tEpsilon: 0.82 \t Action Probabilities: \t [0.18995343 0.19866933 0.2083167  0.1988024  0.20425815] \n",
      "Training 1 Agents on (35,35).\t Episode 200\t Average Score: -0.575\tDones: 67.00%\tEpsilon: 0.67 \t Action Probabilities: \t [0.18736687 0.18408979 0.20825823 0.19261019 0.22767491] \n",
      "Training 1 Agents on (35,35).\t Episode 300\t Average Score: -0.506\tDones: 75.00%\tEpsilon: 0.55 \t Action Probabilities: \t [0.15980719 0.21134594 0.15869485 0.19846125 0.27169077] \n",
      "Training 1 Agents on (35,35).\t Episode 400\t Average Score: -0.499\tDones: 83.00%\tEpsilon: 0.45 \t Action Probabilities: \t [0.16270996 0.2035282  0.15454631 0.2311157  0.24809984] \n",
      "Training 1 Agents on (35,35).\t Episode 500\t Average Score: -0.443\tDones: 84.00%\tEpsilon: 0.37 \t Action Probabilities: \t [0.15251373 0.208703   0.17384875 0.2299324  0.23500211] \n",
      "Training 1 Agents on (35,35).\t Episode 600\t Average Score: -0.414\tDones: 91.00%\tEpsilon: 0.30 \t Action Probabilities: \t [0.16244227 0.22992002 0.15703503 0.22958207 0.22102062] \n",
      "Training 1 Agents on (35,35).\t Episode 700\t Average Score: -0.404\tDones: 89.00%\tEpsilon: 0.25 \t Action Probabilities: \t [0.13949735 0.20278995 0.18653447 0.21420337 0.25697487] \n",
      "Training 1 Agents on (35,35).\t Episode 800\t Average Score: -0.371\tDones: 89.00%\tEpsilon: 0.20 \t Action Probabilities: \t [0.08749686 0.27065026 0.17022345 0.20461963 0.26700979] \n",
      "Training 1 Agents on (35,35).\t Episode 900\t Average Score: -0.347\tDones: 92.00%\tEpsilon: 0.17 \t Action Probabilities: \t [0.08721241 0.24531835 0.15663456 0.18044409 0.33039058] \n",
      "Training 1 Agents on (35,35).\t Episode 1000\t Average Score: -0.326\tDones: 96.00%\tEpsilon: 0.14 \t Action Probabilities: \t [0.11150518 0.224429   0.1655554  0.18910484 0.30940559] \n",
      "Training 1 Agents on (35,35).\t Episode 1100\t Average Score: -0.352\tDones: 93.00%\tEpsilon: 0.11 \t Action Probabilities: \t [0.06290386 0.22576817 0.15310563 0.16180931 0.39641303] \n",
      "Training 1 Agents on (35,35).\t Episode 1200\t Average Score: -0.310\tDones: 99.00%\tEpsilon: 0.09 \t Action Probabilities: \t [0.07179029 0.25781948 0.19496574 0.1808162  0.29460828]  \n",
      "Training 1 Agents on (35,35).\t Episode 1300\t Average Score: -0.296\tDones: 97.00%\tEpsilon: 0.07 \t Action Probabilities: \t [0.04839213 0.23805807 0.19684671 0.18591945 0.33078364] \n",
      "Training 1 Agents on (35,35).\t Episode 1400\t Average Score: -0.287\tDones: 99.00%\tEpsilon: 0.06 \t Action Probabilities: \t [0.04531576 0.26819862 0.1910654  0.18929777 0.30612245] \n",
      "Training 1 Agents on (35,35).\t Episode 1500\t Average Score: -0.308\tDones: 95.00%\tEpsilon: 0.05 \t Action Probabilities: \t [0.11344853 0.22449286 0.17325319 0.18572502 0.30308039] \n",
      "Training 1 Agents on (35,35).\t Episode 1600\t Average Score: -0.296\tDones: 95.00%\tEpsilon: 0.04 \t Action Probabilities: \t [0.05997189 0.27830704 0.18335155 0.19459628 0.28377323] \n",
      "Training 1 Agents on (35,35).\t Episode 1700\t Average Score: -0.289\tDones: 96.00%\tEpsilon: 0.03 \t Action Probabilities: \t [0.03638685 0.24050431 0.22406639 0.23555697 0.26348548] \n",
      "Training 1 Agents on (35,35).\t Episode 1800\t Average Score: -0.274\tDones: 97.00%\tEpsilon: 0.03 \t Action Probabilities: \t [0.0676288  0.23863064 0.19063601 0.18845444 0.31465011]  \n",
      "Training 1 Agents on (35,35).\t Episode 1900\t Average Score: -0.249\tDones: 96.00%\tEpsilon: 0.02 \t Action Probabilities: \t [0.06682666 0.28872069 0.18589625 0.19863393 0.25992247] \n",
      "Training 1 Agents on (35,35).\t Episode 2000\t Average Score: -0.234\tDones: 97.00%\tEpsilon: 0.02 \t Action Probabilities: \t [0.02988281 0.31914063 0.19414063 0.24492188 0.21191406] \n",
      "Training 1 Agents on (35,35).\t Episode 2100\t Average Score: -0.236\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.07989141 0.29823541 0.17878612 0.21562924 0.22745782]  \n",
      "Training 1 Agents on (35,35).\t Episode 2200\t Average Score: -0.287\tDones: 89.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.03542103 0.23329577 0.14442119 0.19626469 0.39059733] \n",
      "Training 1 Agents on (35,35).\t Episode 2300\t Average Score: -0.273\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.02109349 0.28332771 0.18461019 0.26442794 0.24654067] \n",
      "Training 1 Agents on (35,35).\t Episode 2400\t Average Score: -0.292\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.01850973 0.27432368 0.18066762 0.22085113 0.30564784] \n",
      "Training 1 Agents on (35,35).\t Episode 2500\t Average Score: -0.254\tDones: 92.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.03240992 0.29078399 0.14684049 0.20803911 0.32192649] \n",
      "Training 1 Agents on (35,35).\t Episode 2600\t Average Score: -0.265\tDones: 91.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.02360701 0.28988023 0.18781462 0.16993578 0.32876237] \n",
      "Training 1 Agents on (35,35).\t Episode 2700\t Average Score: -0.297\tDones: 90.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00607761 0.30699704 0.16425121 0.15271934 0.36995481] \n",
      "Training 1 Agents on (35,35).\t Episode 2800\t Average Score: -0.234\tDones: 92.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.04108785 0.34122481 0.24417922 0.18626492 0.1872432 ] \n",
      "Training 1 Agents on (35,35).\t Episode 2900\t Average Score: -0.247\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.0466023  0.33698478 0.23264018 0.20516153 0.17861121] \n",
      "Training 1 Agents on (35,35).\t Episode 3000\t Average Score: -0.254\tDones: 93.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.06752353 0.33435916 0.20456191 0.19170891 0.20184649] \n",
      "Training 1 Agents on (35,35).\t Episode 3100\t Average Score: -0.250\tDones: 90.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.09102611 0.31923501 0.16826039 0.14987128 0.27160721] \n",
      "Training 1 Agents on (35,35).\t Episode 3200\t Average Score: -0.261\tDones: 91.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.02084437 0.32644409 0.22981805 0.15015015 0.27274333] \n",
      "Training 1 Agents on (35,35).\t Episode 3300\t Average Score: -0.212\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.04020641 0.35067727 0.25628897 0.19694689 0.15588046] \n",
      "Training 1 Agents on (35,35).\t Episode 3400\t Average Score: -0.217\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.044      0.37789474 0.27221053 0.23094737 0.07494737] \n",
      "Training 1 Agents on (35,35).\t Episode 3500\t Average Score: -0.202\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.03178539 0.39472498 0.2157349  0.29147881 0.06627592] \n",
      "Training 1 Agents on (35,35).\t Episode 3600\t Average Score: -0.207\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.03330393 0.37141597 0.27062197 0.23114248 0.09351566] \n",
      "Training 1 Agents on (35,35).\t Episode 3700\t Average Score: -0.226\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.05751316 0.36046983 0.25496152 0.23795059 0.0891049 ] \n",
      "Training 1 Agents on (35,35).\t Episode 3800\t Average Score: -0.224\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.02473932 0.33592312 0.2596606  0.23737477 0.14230219] \n",
      "Training 1 Agents on (35,35).\t Episode 3900\t Average Score: -0.300\tDones: 87.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00741886 0.32534776 0.24744977 0.24791345 0.17187017] \n",
      "Training 1 Agents on (35,35).\t Episode 4000\t Average Score: -0.234\tDones: 92.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00274833 0.40321947 0.26305457 0.2699254  0.06105222] \n",
      "Training 1 Agents on (35,35).\t Episode 4100\t Average Score: -0.209\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.0176586  0.43012862 0.24089819 0.25376063 0.05755396] \n",
      "Training 1 Agents on (35,35).\t Episode 4200\t Average Score: -0.220\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00892857 0.43770764 0.28862126 0.23525748 0.02948505] \n",
      "Training 1 Agents on (35,35).\t Episode 4300\t Average Score: -0.244\tDones: 92.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.02369757 0.36185819 0.26086139 0.19898439 0.15459846] \n",
      "Training 1 Agents on (35,35).\t Episode 4400\t Average Score: -0.246\tDones: 92.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00560433 0.42387446 0.25312909 0.23949187 0.07790024] \n",
      "Training 1 Agents on (35,35).\t Episode 4500\t Average Score: -0.260\tDones: 89.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.0060209  0.38498318 0.23906499 0.30759695 0.06233398] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Agents on (35,35).\t Episode 4600\t Average Score: -0.244\tDones: 92.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.03534499 0.40797142 0.25982328 0.23463057 0.06222974] \n",
      "Training 1 Agents on (35,35).\t Episode 4700\t Average Score: -0.205\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00933541 0.43920871 0.24072016 0.21160258 0.09913314] \n",
      "Training 1 Agents on (35,35).\t Episode 4800\t Average Score: -0.240\tDones: 92.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.03635668 0.42345962 0.25143513 0.21794872 0.07079985] \n",
      "Training 1 Agents on (35,35).\t Episode 4900\t Average Score: -0.251\tDones: 91.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00878317 0.43970723 0.25928637 0.25562672 0.03659652] \n",
      "Training 1 Agents on (35,35).\t Episode 5000\t Average Score: -0.222\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.05350895 0.39164437 0.2132126  0.19386705 0.14776703] \n",
      "Training 1 Agents on (35,35).\t Episode 5100\t Average Score: -0.259\tDones: 88.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00764852 0.43863394 0.23443614 0.2282106  0.09107079] \n",
      "Training 1 Agents on (35,35).\t Episode 5200\t Average Score: -0.252\tDones: 93.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.03605901 0.4634857  0.22054271 0.23201603 0.04789656] \n",
      "Training 1 Agents on (35,35).\t Episode 5300\t Average Score: -0.240\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.02695469 0.44159816 0.24870962 0.21162302 0.07111451] \n",
      "Training 1 Agents on (35,35).\t Episode 5400\t Average Score: -0.214\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.05010661 0.48784648 0.18251599 0.23965885 0.03987207] \n",
      "Training 1 Agents on (35,35).\t Episode 5500\t Average Score: -0.219\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.065335   0.47565543 0.24198918 0.20952975 0.00749064] \n",
      "Training 1 Agents on (35,35).\t Episode 5600\t Average Score: -0.217\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.05945378 0.49915966 0.18823529 0.20714286 0.0460084 ] \n",
      "Training 1 Agents on (35,35).\t Episode 5700\t Average Score: -0.222\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.03970376 0.5157375  0.17486114 0.21518206 0.05451553] \n",
      "Training 1 Agents on (35,35).\t Episode 5800\t Average Score: -0.239\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.08802614 0.50855276 0.18892946 0.20372862 0.01076302] \n",
      "Training 1 Agents on (35,35).\t Episode 5900\t Average Score: -0.202\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.01781687 0.50405954 0.18899414 0.21357691 0.07555255] \n",
      "Training 1 Agents on (35,35).\t Episode 6000\t Average Score: -0.223\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.01560575 0.53675565 0.21190965 0.23449692 0.00123203] \n",
      "Training 1 Agents on (35,35).\t Episode 6100\t Average Score: -0.195\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00582072 0.52922002 0.18742724 0.19161816 0.08591385] \n",
      "Training 1 Agents on (35,35).\t Episode 6200\t Average Score: -0.189\tDones: 98.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00239866 0.57183977 0.21252099 0.1942912  0.01894939] \n",
      "Training 1 Agents on (35,35).\t Episode 6300\t Average Score: -0.191\tDones: 98.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00189484 0.56963524 0.21601137 0.20606348 0.00639507] \n",
      "Training 1 Agents on (35,35).\t Episode 6400\t Average Score: -0.211\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00906149 0.59525351 0.18554477 0.20884574 0.0012945 ] \n",
      "Training 1 Agents on (35,35).\t Episode 6500\t Average Score: -0.223\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.08617152 0.55826836 0.16495691 0.18937218 0.00123102] \n",
      "Training 1 Agents on (35,35).\t Episode 6600\t Average Score: -0.222\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.01811072 0.54970158 0.24840502 0.17328668 0.01049599] \n",
      "Training 1 Agents on (35,35).\t Episode 6700\t Average Score: -0.196\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.05183985 0.59106688 0.18699375 0.16894237 0.00115714] \n",
      "Training 1 Agents on (35,35).\t Episode 6800\t Average Score: -0.193\tDones: 98.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.06015038 0.55615602 0.22532895 0.15765977 0.00070489] \n",
      "Training 1 Agents on (35,35).\t Episode 6900\t Average Score: -0.176\tDones: 99.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.02508318 0.57563348 0.21448682 0.16534425 0.01945227]  \n",
      "Training 1 Agents on (35,35).\t Episode 7000\t Average Score: -0.213\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.0888651  0.49443255 0.21905782 0.14989293 0.04775161] \n",
      "Training 1 Agents on (35,35).\t Episode 7100\t Average Score: -0.189\tDones: 99.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.06028708 0.56602871 0.23253589 0.13995215 0.00119617] \n",
      "Training 1 Agents on (35,35).\t Episode 7200\t Average Score: -0.182\tDones: 99.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.0077172  0.59073936 0.23823749 0.14712472 0.01618123] \n",
      "Training 1 Agents on (35,35).\t Episode 7300\t Average Score: -0.187\tDones: 98.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.01141331 0.55075279 0.26663429 0.13890238 0.03229723]  \n",
      "Training 1 Agents on (35,35).\t Episode 7400\t Average Score: -0.198\tDones: 98.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.03962437 0.54626661 0.26339899 0.12620247 0.02450756] \n",
      "Training 1 Agents on (35,35).\t Episode 7500\t Average Score: -0.233\tDones: 92.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.04113364 0.51899232 0.22869514 0.16177918 0.04939972] \n",
      "Training 1 Agents on (35,35).\t Episode 7600\t Average Score: -0.182\tDones: 98.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.01066468 0.59598214 0.25719246 0.13516865 0.00099206] \n",
      "Training 1 Agents on (35,35).\t Episode 7700\t Average Score: -0.205\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00886721 0.59654179 0.25892263 0.13478164 0.00088672] \n",
      "Training 1 Agents on (35,35).\t Episode 7800\t Average Score: -0.198\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00229621 0.5880597  0.25579793 0.11343284 0.04041332] \n",
      "Training 1 Agents on (35,35).\t Episode 7900\t Average Score: -0.208\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00677892 0.6133829  0.22742182 0.13929587 0.01312049] \n",
      "Training 1 Agents on (35,35).\t Episode 8000\t Average Score: -0.199\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.0532085  0.55469285 0.21625942 0.12788308 0.04795615] \n",
      "Training 1 Agents on (35,35).\t Episode 8100\t Average Score: -0.240\tDones: 91.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.05785441 0.58467433 0.2256705  0.12241379 0.00938697] \n",
      "Training 1 Agents on (35,35).\t Episode 8200\t Average Score: -0.203\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.07040359 0.58340807 0.19663677 0.1206278  0.02892377] \n",
      "Training 1 Agents on (35,35).\t Episode 8256\t Average Score: -0.234\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [7.80762024e-03 6.27420362e-01 2.36102436e-01 1.28044972e-01\n",
      "Training 1 Agents on (35,35).\t Episode 8257\t Average Score: -0.234\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [7.75193798e-03 6.26356589e-01 2.37829457e-01 1.27441860e-01\n",
      "Training 1 Agents on (35,35).\t Episode 8258\t Average Score: -0.234\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [7.64993880e-03 6.27600979e-01 2.35312118e-01 1.28824969e-01\n",
      "Training 1 Agents on (35,35).\t Episode 8300\t Average Score: -0.239\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.0091954  0.60708812 0.22452107 0.12298851 0.0362069 ] \n",
      "Training 1 Agents on (35,35).\t Episode 8400\t Average Score: -0.194\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.01474719 0.59269663 0.19382022 0.10252809 0.09620787] \n",
      "Training 1 Agents on (35,35).\t Episode 8500\t Average Score: -0.257\tDones: 93.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.02128421 0.61384368 0.23877661 0.12430692 0.00178859] \n",
      "Training 1 Agents on (35,35).\t Episode 8600\t Average Score: -0.247\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.006684   0.61269959 0.22428518 0.13238025 0.02395098] \n",
      "Training 1 Agents on (35,35).\t Episode 8700\t Average Score: -0.186\tDones: 98.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.04169715 0.65471836 0.1923921  0.10436479 0.0068276 ] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Agents on (35,35).\t Episode 8800\t Average Score: -0.193\tDones: 98.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00987306 0.64809591 0.20427833 0.13563705 0.00211566] \n",
      "Training 1 Agents on (35,35).\t Episode 8900\t Average Score: -0.205\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00444741 0.65265733 0.23904825 0.10295753 0.00088948] \n",
      "Training 1 Agents on (35,35).\t Episode 9000\t Average Score: -0.256\tDones: 93.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.01687915 0.59597773 0.21009158 0.12138625 0.05566529] \n",
      "Training 1 Agents on (35,35).\t Episode 9032\t Average Score: -0.244\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.43406593e-03 6.94368132e-01 1.72390110e-01 1.29120879e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9033\t Average Score: -0.254\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.60144058e-03 6.93877551e-01 1.78871549e-01 1.23049220e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9034\t Average Score: -0.255\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.51493849e-03 6.91857059e-01 1.82776801e-01 1.21265378e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9035\t Average Score: -0.254\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.45423143e-03 6.85664940e-01 1.84801382e-01 1.25503742e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9036\t Average Score: -0.254\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.37078652e-03 6.86516854e-01 1.84831461e-01 1.24719101e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9100\t Average Score: -0.238\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00520532 0.58492385 0.16830538 0.12916908 0.11239638] \n",
      "Training 1 Agents on (35,35).\t Episode 9200\t Average Score: -0.233\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.01903827 0.66162905 0.17801766 0.11736997 0.02394504] \n",
      "Training 1 Agents on (35,35).\t Episode 9228\t Average Score: -0.251\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [2.45573958e-02 6.16219303e-01 1.91319246e-01 1.67332953e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9229\t Average Score: -0.251\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [2.40761478e-02 6.20940649e-01 1.89249720e-01 1.65173572e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9230\t Average Score: -0.242\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [2.37306843e-02 6.23068433e-01 1.87086093e-01 1.65562914e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9231\t Average Score: -0.234\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [2.31681034e-02 6.21767241e-01 1.92349138e-01 1.62176724e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9232\t Average Score: -0.235\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [2.26315789e-02 6.24736842e-01 1.92631579e-01 1.59473684e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9300\t Average Score: -0.233\tDones: 94.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.0153332  0.59838805 0.19638294 0.13839198 0.05150383] \n",
      "Training 1 Agents on (35,35).\t Episode 9400\t Average Score: -0.193\tDones: 98.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00891182 0.684803   0.18409944 0.12124765 0.00093809] \n",
      "Training 1 Agents on (35,35).\t Episode 9500\t Average Score: -0.213\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00513369 0.63700535 0.19786096 0.14203209 0.01796791]  \n",
      "Training 1 Agents on (35,35).\t Episode 9600\t Average Score: -0.235\tDones: 93.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.01346604 0.59367681 0.22599532 0.13485558 0.03200625] \n",
      "Training 1 Agents on (35,35).\t Episode 9700\t Average Score: -0.227\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00887813 0.63539144 0.17271994 0.16545601 0.01755448] \n",
      "Training 1 Agents on (35,35).\t Episode 9800\t Average Score: -0.221\tDones: 95.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00103434 0.64418701 0.20231692 0.15101365 0.00144808] \n",
      "Training 1 Agents on (35,35).\t Episode 9838\t Average Score: -0.197\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [4.23472474e-03 6.05565638e-01 2.11736237e-01 1.77858439e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9839\t Average Score: -0.196\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [4.17910448e-03 6.03582090e-01 2.12537313e-01 1.79104478e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9840\t Average Score: -0.197\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [4.12249706e-03 6.04829211e-01 2.13780919e-01 1.76678445e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9841\t Average Score: -0.199\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.95927602e-03 6.11990950e-01 2.08710407e-01 1.74773756e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9842\t Average Score: -0.199\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.91498881e-03 6.10178971e-01 2.11968680e-01 1.73378076e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9843\t Average Score: -0.198\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.86313466e-03 6.13686534e-01 2.10264901e-01 1.71633554e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9844\t Average Score: -0.196\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.82304752e-03 6.12779902e-01 2.11359913e-01 1.71490989e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9845\t Average Score: -0.188\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.73731981e-03 6.12920448e-01 2.10891618e-01 1.71916711e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9846\t Average Score: -0.189\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.66492147e-03 6.16230366e-01 2.10994764e-01 1.68586387e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9847\t Average Score: -0.187\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.63258952e-03 6.17540218e-01 2.10690192e-01 1.67618059e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9848\t Average Score: -0.186\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.59342916e-03 6.14476386e-01 2.09958932e-01 1.71457906e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9849\t Average Score: -0.186\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.50701403e-03 6.18236473e-01 2.09919840e-01 1.67835671e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9850\t Average Score: -0.185\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.45679012e-03 6.16790123e-01 2.12839506e-01 1.66419753e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9851\t Average Score: -0.184\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.42633382e-03 6.17719041e-01 2.11943221e-01 1.66421929e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9852\t Average Score: -0.185\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.27868852e-03 6.25761124e-01 2.08899297e-01 1.61592506e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9853\t Average Score: -0.187\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.63967243e-03 6.24203822e-01 2.11555960e-01 1.60145587e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9854\t Average Score: -0.187\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.58422939e-03 6.26344086e-01 2.11021505e-01 1.58602151e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9855\t Average Score: -0.187\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.55555556e-03 6.25777778e-01 2.10222222e-01 1.60000000e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9856\t Average Score: -0.188\tDones: 97.00%\tEpsilon: 0.01 \t Action Probabilities: \t [3.48432056e-03 6.25000000e-01 2.13850174e-01 1.57229965e-01\n",
      "Training 1 Agents on (35,35).\t Episode 9900\t Average Score: -0.231\tDones: 93.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.00278164 0.61215975 0.19948341 0.14583747 0.03973773] \n",
      "Training 1 Agents on (35,35).\t Episode 10000\t Average Score: -0.228\tDones: 96.00%\tEpsilon: 0.01 \t Action Probabilities: \t [0.0112157  0.61125576 0.2165031  0.15982375 0.00120168] \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
